{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZ5RH0LHys5DzGJBu6Ei9q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyunwoo126/activeloop/blob/main/LangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain==0.0.152 deeplake openai tiktoken pyllamacpp==1.0.7 sentencepiece gpt4all"
      ],
      "metadata": {
        "id": "94V8iVep2qVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download GPT4All (This process might take a while since the file size is 4GB)"
      ],
      "metadata": {
        "id": "a6q24sem4Hfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "local_path = './models/gpt4all-lora-quantized-ggml.bin'\n",
        "Path(local_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "url = 'https://the-eye.eu/public/AI/models/nomic-ai/gpt4all/gpt4all-lora-quantized-ggml.bin'\n",
        "\n",
        "# send a GET request to the URL to download the file.\n",
        "response = requests.get(url, stream=True)\n",
        "\n",
        "# open the file in binary mode and write the contents of the response\n",
        "# to it in chunks.\n",
        "with open(local_path, 'wb') as f:\n",
        "    for chunk in tqdm(response.iter_content(chunk_size=8192)):\n",
        "        if chunk:\n",
        "            f.write(chunk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amWnTe9b4GTD",
        "outputId": "b319d64d-9c64-4b7c-9f94-389a702d9515"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "514266it [03:29, 2452.40it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "it is time to transform the downloaded file to the latest format. We start by downloading the codes in the LLaMAcpp repository or simply fork it using the following command. (You need to have the git command installed) Pass the downloaded file to the convert.py script and run it with a Python interpreter."
      ],
      "metadata": {
        "id": "uQngIW5Z5O5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf llama.cpp/\n",
        "!git clone https://github.com/ggerganov/llama.cpp.git\n",
        "!python3 llama.cpp/convert.py ./models/gpt4all-lora-quantized-ggml.bin"
      ],
      "metadata": {
        "id": "x1sEsZg85KgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import GPT4All"
      ],
      "metadata": {
        "id": "XujMZt_E55EG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyllamacpp==1.0.7"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fVFcOwS_X7P",
        "outputId": "8246a27c-6912-4cad-d900-0b62ba8d70da"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyllamacpp==1.0.7 in /usr/local/lib/python3.10/dist-packages (1.0.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import GPT4All\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.callbacks.base import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
      ],
      "metadata": {
        "id": "qWlRDTh_54YV"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup GPT4All llm instead of OpenAI"
      ],
      "metadata": {
        "id": "ooazRFNk6Uo0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gpt4all"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMIKPX5yFDoD",
        "outputId": "32c0e81d-9369-46e1-f129-4e7c45244fc7"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gpt4all\n",
            "  Downloading gpt4all-0.3.6-py3-none-manylinux1_x86_64.whl (2.5 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m1.8/2.5 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from gpt4all) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gpt4all) (4.65.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all) (3.4)\n",
            "Installing collected packages: gpt4all\n",
            "Successfully installed gpt4all-0.3.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "tD4SjIzE2WXf",
        "outputId": "f746ae7f-c57a-4823-a43c-b4bf9659248b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Question: What happens when it rains somewhere?\n",
            "\n",
            "Answer: Let's think step by step. When rain occurs, water vapor rises from the Earth’s surface and meets with air in a column above it (weather layer). This process of lifting is called convection. Water droplets or ice crystals form around particles that are found nearer to earth's Surface due to its higher temperature. The smaller, denser water vapor rises upwards and collides with larger raindrops in the atmosphere forming clouds which gives rise to a rainbow (arching rainbows). When these droplets or ice crystals collide they create snowflakes, hailstones etc., If it is too cold for water vapor to condense into liquid form and fall as precipitation, then the cloud would likely dissipate. As a result of this process we get lightning which causes thunder."
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Question: What happens when it rains somewhere?\\n\\nAnswer: Let's think step by step. When rain occurs, water vapor rises from the Earth’s surface and meets with air in a column above it (weather layer). This process of lifting is called convection. Water droplets or ice crystals form around particles that are found nearer to earth's Surface due to its higher temperature. The smaller, denser water vapor rises upwards and collides with larger raindrops in the atmosphere forming clouds which gives rise to a rainbow (arching rainbows). When these droplets or ice crystals collide they create snowflakes, hailstones etc., If it is too cold for water vapor to condense into liquid form and fall as precipitation, then the cloud would likely dissipate. As a result of this process we get lightning which causes thunder.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
        "\n",
        "# instead of llm = OpenAI(model=\"text-davinci-003\", temperature=0.9)\n",
        "llm = GPT4All(model=\"./models/ggml-model-q4_0.bin\", callback_manager=callback_manager, verbose=True)\n",
        "# llm = GPT4All(model=\"./models/ggml-model-q4_0.bin\", verbose=True)\n",
        "\n",
        "\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "question = \"What happens when it rains somewhere?\"\n",
        "llm_chain.run(question)"
      ]
    }
  ]
}